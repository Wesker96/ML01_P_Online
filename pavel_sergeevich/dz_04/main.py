from collections import Counter

import util

"""
1.	Загрузить файл длиной не менее 2000 символов. 
2.	Составить программу, которая считает число уникальных слов в тексте (без критерия схожести)
3.	Составить программу, которая считает число гласных и согласных букв. 
4.	Составить программу, которая считает число предложений, их длину и число (количество) раз использования каждого
    слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy
    (en_core_web_sm) или расстояние Левенштейна). 
5.	Вывести 10 наиболее часто встречаемых слов.

Чуть более глубокое описание темы оставил в конспекте: lessons/Урок 4.md

Выводы: интересное задание, в  первую очередь тем, что приоткрывает дверцу в NLP. Немного осовоил spacy, разобрался в 
лемматизации (приведение слова к исходной форме) и стемминге (поиск основы слова), почитал про POST и самое главное
провел свой первый анализ текста. Узнал, что есть стоп-слова
и их для чистоты нужно убирать из набора. Это же касается и очень редких слов. Объясняется это
эмпирическим законом Ципфа. Но при этом нужно быть аккуртаным - учитывать решаемую задачу - например при анализе
тональности текста удаление частицы "не" может влиять на результат.

"""

# 1. Загрузить файл длиной не менее 2000 символов.
txt_raw = util.load_fl("book.txt")
print("Загрузка книги. Количество символов: " + str(len(txt_raw)))

txt_clean, txt_tk = util.tokenizer(txt_raw)
print("Количество токенов: " + str(len(txt_tk)))
# print(txt_tk)

# 2. Составить программу, которая считает число уникальных слов в тексте (без критерия схожести)
txt_tk_set = set(txt_tk)
print("Количество уникальных токенов (слов): " + str(len(txt_tk_set)))

# 3. Составить программу, которая считает число гласных и согласных букв.
word = txt_tk[20]
v_cnt, c_cnt = util.get_type_of_letter(word)
print("Количество гластных букв в слове \"" + word + "\" " + str(v_cnt) + ", а согласных - " + str(c_cnt))

general_v_cnt = 0
general_c_cnt = 0
for w in txt_tk:
    v_cnt, c_cnt = util.get_type_of_letter(w)
    general_v_cnt += v_cnt
    general_c_cnt += c_cnt
print("Количество гластных букв всех токенов " + str(general_v_cnt) + ", а согласных - " + str(general_c_cnt))

# 4. Составить программу, которая считает число предложений, их длину и число (количество) раз использования каждого
# слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm)
# или расстояние Левенштейна).

# очищаю текст от лишних символов за исключением знаков препинания, после, разделяю текст по знакам препинания
sentence = util.sentence_tokenizer(txt_raw)
print("Количество предложений в тексте " + str(len(sentence)))

# формирую массивы уникальных лемм
# lemma_uniq - со стоповыми словами
# lemma_uniq_without_sw - без стоповых слов

# Выдержка из конспекта:
# Короче, говоря, из обзора [инструментов NLP], я заключаю, что SpaCy отличный выбор для решения задачи, главным
# образом потому что это
# работает не хуже остальных инструментов (или лучше), есть масса примеров, хорошо работает на процессоре локальной
# машины (кстати на GPU она тоже работает), есть поддержка русского языка, наконец используется в продакшене. Если же
# целью является исследование, лучше использовать NLTK.

lemma_uniq, lemma_uniq_without_sw = util.spacy_processing(txt_clean)

# применяю встроенный Counter подкласс словаря dict для подсчета хеш-объектов
lemma_uniq_cn = Counter(lemma_uniq)
lemma_uniq_without_sw_cn = Counter(lemma_uniq_without_sw)

lemma_uniq = set(lemma_uniq)
lemma_uniq_without_sw = set(lemma_uniq_without_sw)

print("Количество уникальных лемм: " + str(len(lemma_uniq)))
print("Количество уникальных лемм исключая стоп-слова: " + str(len(lemma_uniq_without_sw)))
print("Разница между списками лемм: " + str(len(lemma_uniq) - len(lemma_uniq_without_sw)))

# print(stop_words.STOP_WORDS)

# 5. Вывести 10 наиболее часто встречаемых слов.
max_len_num = 0
print("Список наиболее часто встречаемых слов в тексте: ")
for tk in lemma_uniq_without_sw_cn.most_common(10):
    if max_len_num == 0:
        max_len_num = len(str(tk[1]))

    print("     - " + str(tk[1]) + " " * (max_len_num - len(str(tk[1]))) + " (" + tk[0] + ")")

"""
Out:
Загрузка книги. Количество символов: 57929
Количество токенов: 8616
Количество уникальных токенов (слов): 3443
Количество гластных букв в слове "провел" 2, а согласных - 4
Количество гластных букв всех токенов 18716, а согласных - 26874
Количество предложений в тексте 1144
Количество уникальных лемм: 2614
Количество уникальных лемм исключая стоп-слова: 2361
Разница между списками лемм: 253
Список наиболее часто встречаемых слов в тексте: 
     - 196 (вельт)
     - 119 (илья)
     - 50  (кристалл)
     - 47  (планета)
     - 39  (глаз)
     - 38  (белый)
     - 35  (убить)
     - 34  (снег)
     - 34  (рука)
     - 30  (вельта)
"""