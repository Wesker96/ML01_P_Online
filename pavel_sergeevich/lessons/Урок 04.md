Разбирали базовые основы Python.

Нашел ссылку по Python: https://github.com/GnuriaN/Python-Roadmap 

---

#### Домашнее задание

И тааак. Есть два основных понятия, которые нужно усвоить в теме дз. Стемминг и лемматизация. Пока что начнем с вики...

**Стемминг** - процесс нахождения основы слова для заданного исходного слова. Причем основа слова вовсе не должна совпадать с его корнем. Этот процесс применяется в поисковых системах и является частью процесса нормализации слова - процесс приведения текста к некой стандартной, называемой канонической, форме представления.

Алгоритмы стемминга:
- алгоритмы поиска - поиск флективной (???) формы по таблице, во избежание ошибки совпадения разных слов по лемме, применяют частеречную разметку, суть которой сводится к расстановке тегов части речи к словам (POS-tagging).
- усечение окончаний - суть этого подхода сводится к набору неких правил построения слов. Эти алгоритмы работают хорошо, когда есть внятные лингвистические правила. Плохо отрабатываются исключительные ситуации.
- аффикс-стеммеры - аффикс это морфема, как наиболее распространенные - суффикс и префикс. Иными словами, эти алгоритмы работают с морфемами.
- лемматизация - приведение слова к исходной форме, включает в себя определение части речи и применение правил нормализации текста.
- подход ripple-down rules - пока не понятно...
- стохастические алгоритмы - построение вероятностной модели на основе таблицы соответствия корневых и флективных форм.
- анализ N-грамм - построение последовательностей (вероятностные модели)

Токенизация - разделение текста на какие-то единицы. Методов токенизации очень много.

Стоп-слово — обычно это служебная часть речи, которая употребляется очень часто и не вносит в текст какой-либо особый смысл.
Удалять стоп-слова имеет смысл потому что — закон Ципфа (эмпирический) — частота слова примерно обратно пропорциональна его рангу, где ранг это порядковый номер слова. Объяснение этого закона, основанное на корреляционных свойствах аддитивных марковских цепей было дано в 2005, хотя сам закон был впервые описан аж в 1908 году.

При удалении стоп-слов нужно быть аккуратным и в этом процессе нужно опираться на решаемую задачу. Например, частица "не" находится в списке стоп-слов, однако ее удаление может повлиять на анализ тональности текста.

Нашел хороший [хэндбук](https://education.yandex.ru/handbook/data-analysis/article/preobrazovanie-tekstovyh-dannyh-i-rabota-s-nimi-v-python) для выполнения ДЗ.

Коротко по инструментам.
 - NLTK (Natural Language Toolkit) — комбайн для исследований в направлении NLP. Кстати, набор стоп-слов в этой библиотеке составляет 151 слова. Судя по гитхабу полностью написан на Python, что сказывается на скорости работы.
 - SpaCy — примерно тоже что и NLTK, но уже больше для реализации рабочих решений, а не исследований. Написан преимущественно на Python и Cython. А вот этого инструмента список стоп-слов составляется аж 768 единиц. Не уверен, что больше обязательно лучше.
 - Spark NLP — на сколько я понимаю, основной упор этого инструмента приходится на масштабируемость в распределенной среде. Написан преимущественно на Scala
 - Stanford CoreNLP — нечто написанное на Java.
 - [Stanza](https://github.com/stanfordnlp/stanza) — полностью на писан на Python. Включает py-обертку над Stanford CoreNLP. [Говорят](https://habr.com/ru/articles/504680/#comment_21686190), пошустрее SpaCy.
 - Flair — полностью на писан на Python. Имеется какая-то специальная поддержка для биомедицинских текстов. На huggingface есть огромная куча всяких моделек, но русского не нашел.

Короче, говоря, из обзора, я заключаю, что SpaCy отличный выбор для решения задачи, главным образом потому что это работает не хуже остальных инструментов (или лучше), есть масса примеров, хорошо работает на процессоре локальной машины (кстати на GPU она тоже работает), есть поддержка русского языка, наконец используется в продакшене. Если же целью является исследование, лучше использовать NLTK.

