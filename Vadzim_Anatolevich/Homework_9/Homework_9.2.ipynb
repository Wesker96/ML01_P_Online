{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05a250a-c0c9-48bd-986e-6359dad950ea",
   "metadata": {},
   "source": [
    "# Домашняя работа №9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6d9d1-99e5-4cbe-894a-5052a7de1520",
   "metadata": {},
   "source": [
    "## Цели и задачи работы:\n",
    "1) Написать алгоритм, который принимает на вход любую видеопоследовательность;\n",
    "2) В данной видеопоследовательности необходимо реализовать и зафиксировать на первых кадрах две точки: точку в центре экрана и опорную точку;\n",
    "3) Вычислить координаты данных точек на каждом кадре видеопоследовательности и рассчитать ошибку определения опорной точки для каждого кадра;\n",
    "4) Оценить, какой детектор лучше подходит для определения опорных точек на каждом кадре видеопоследовательности;\n",
    "5) Наложить фильтр шумов на видеопоследовательность и оценить ошибку определения опорной точки для каждого кадра с шумами;\n",
    "6) Наложить фильтр для повышения контрастности на видеопоследовательность;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcd4c1-7117-4117-ad69-c9f44d200c4d",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0816d8-8597-4727-b17e-19914b7d55f7",
   "metadata": {},
   "source": [
    "В качестве видеопоследовательности будет использоваться 15-ти секундное видео с полетом дрона."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df9298-389d-444c-82ad-8073451404af",
   "metadata": {},
   "source": [
    "Для дальнейших манипуляций с видеопоследовательностью импортируем следующие библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6703db23-996c-488f-a276-c6b027700a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e187eb-a731-4349-8913-c85e740318c3",
   "metadata": {},
   "source": [
    "# 1.Реализация алгоритма вычисления координат точек на кадрах видеопотока и ошибки их определения "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046cec3-ce0b-4b74-996d-16b4568577fe",
   "metadata": {},
   "source": [
    "Реализуем функцию \"track_video_function()\", которая принимает на вход следующие аргументы:\n",
    "1) \"input_video\" - путь к видеофайлу для обработки;\n",
    "2) \"output_video\" - путь, куда будет сохранен видеофайл после обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34594db5-6f4b-47a4-bb5c-565f6a70ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_video_function(input_video, output_video):\n",
    "    print(\"Начало обработки видеопотока...\");\n",
    "    capture = cv2.VideoCapture(input_video);\n",
    "    video_fps = capture.get(cv2.CAP_PROP_FPS);\n",
    "    video_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH));\n",
    "    video_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT));\n",
    "    video_codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output = cv2.VideoWriter(output_video, video_codec, video_fps, (video_width, video_height));\n",
    "    retur, first_frame = capture.read();\n",
    "    \n",
    "    if not retur:\n",
    "        print(f\"Не удается прочитать видеофайл {input_video}\");\n",
    "        return;\n",
    "\n",
    "    detector_type = cv2.ORB_create();\n",
    "    keypoints, descriptors = detector_type.detectAndCompute(first_frame, None);\n",
    "\n",
    "    if len(keypoints) > 0:\n",
    "        initial_point = keypoints[290].pt;\n",
    "    \n",
    "    else:\n",
    "        print(\"Ключевые точки не найдены\");\n",
    "        return;\n",
    "\n",
    "    points_to_track = np.array([[initial_point]], dtype = np.float32);\n",
    "    gray_map_initial = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY);\n",
    "    center_point = (video_width // 2, video_height // 2);\n",
    "    output.write(first_frame);\n",
    "    trajectories = [];\n",
    "\n",
    "    while True:\n",
    "        retur, current_frame = capture.read();\n",
    "        \n",
    "        if not retur:\n",
    "            break;\n",
    "\n",
    "        gray_map_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY);\n",
    "        new_points, point_status, error = cv2.calcOpticalFlowPyrLK(gray_map_initial, gray_map_current, points_to_track, None);\n",
    "\n",
    "        if point_status[0] == 1:\n",
    "            tracked_point = new_points[0][0];\n",
    "            trajectories.append((int(tracked_point[0]), int(tracked_point[1])));\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                source_points = np.array(trajectories[-3:], dtype = np.float32);\n",
    "                destination_points = np.array([points_to_track[0][0], tracked_point, tracked_point], dtype = np.float32);\n",
    "                M, inliers = cv2.estimateAffinePartial2D(source_points, destination_points);\n",
    "\n",
    "                if M is not None:\n",
    "                    points_to_track = cv2.transform(points_to_track, M);\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                \n",
    "                for i in range(2, len(trajectories)):\n",
    "                    cv2.line(current_frame, trajectories[i - 1], trajectories[i], (255, 0, 0), 2);\n",
    "\n",
    "            if len(trajectories) > 3:\n",
    "                cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "\n",
    "            cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "            cv2.drawMarker(current_frame, center_point, (0, 255, 0), markerType = cv2.MARKER_CROSS, markerSize = 20, thickness = 2);\n",
    "            end_point = (center_point[0] + 100, center_point[1] + 100);\n",
    "            cv2.line(first_frame, center_point, end_point, (255, 0, 0), 2);\n",
    "            draw_function(current_frame, tracked_point, trajectories, center_point);\n",
    "            points_to_track = new_points.reshape(-1, 1, 2);           \n",
    "        \n",
    "        else:\n",
    "            break;  \n",
    "\n",
    "        output.write(current_frame);\n",
    "        cv2.imshow(\"Video file point tracking in process...\", current_frame);\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break;\n",
    "\n",
    "        gray_map_initial = gray_map_current;\n",
    "        \n",
    "    capture.release();\n",
    "    output.release();\n",
    "    cv2.destroyAllWindows();\n",
    "    print(f\"Обработка видеопотока завершена. Обработанный файл {output_video} сохранен в директорию проекта\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60873340-065d-44d6-aff7-858dd6d6fb7d",
   "metadata": {},
   "source": [
    "Последовательность работы данной функции:\n",
    "1) При помощи функций \"cv2.VideoCapture()\", \"capture.get()\" мы осуществляем чтение видеофайла и получаем ряд его параметров: частоту кадров в секунду, ширину и высоту каждого кадра видеофайла в пикселях; \n",
    "2) При помощи функций \"cv2.VideoWriter_fourcc()\" и \"cv2.VideoWriter()\" выполняется определение кодека видео после обработки и задаются его выходные параметры (путь к выходному видеофайлу, частота видео (кадров/сек) и т.д.).\n",
    "3) Конструкция \"retur, first_frame = capture.read()\" выполняет чтение каждого кадра входного видеофайла. Конструкция: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6922e-1eac-4669-95e8-e158b7ac1b33",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "    if not retur:\n",
    "        print(f\"Не удается прочитать видеофайл {input_video}\");\n",
    "        return;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9021f1-6df9-4e05-bf51-c33b4b416cc7",
   "metadata": {},
   "source": [
    "возвращает \"False\", если не было получено значение \"True\" для параметра \"retur\", который является маркером чтения кадра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fd39f-5e3b-462a-9fc0-6a3833d0e84d",
   "metadata": {},
   "source": [
    "4) Функция \"cv2.ORB_create()\" определяет тип детектора \"ORB\", который будет использоваться по умолчанию для поиска ключевых точек.\n",
    "Функция \"detector_type.detectAndCompute()\" будет получать набор ключевых точек и их дескрипторы для каждого кадра видеопотока.\n",
    "Конструкция:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07861d-3f88-4b2a-93e7-4faf907073c6",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "    if len(keypoints) > 0:\n",
    "        initial_point = keypoints[290].pt;\n",
    "\n",
    "    else:\n",
    "        print(\"Ключевые точки не найдены\");\n",
    "        return;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b0004-d24c-478d-8dbf-97aa6489642e",
   "metadata": {},
   "source": [
    "определяет начальную точку из набора, которая будет использоваться в качестве ключевой. Конструкция возвращает \"False\", если ключевые точки не были определены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798b792-1a10-44e6-aa92-fa3326729b5a",
   "metadata": {},
   "source": [
    "5) Конструкция \"points_to_track = np.array([[initial_point]], dtype = np.float32)\" определяет массив ключевых точек для каждого кадра в формате \"np.float32\". Функция \"cv2.cvtColor()\" переводит каждый кадр в цветовое пространство \"COLOR_BGR2GRAY\" (все цвета на кадре будут являться градациями серого). Конструкция \"center_point = (video_width // 2, video_height // 2)\" определяет центр каждого кадра. Функция \"output.write()\" осуществялет запись каждого кадра в выходной видеофайл. Переменная \"trajectories\" представляет собой пустой массив для хранения координат опорной ключевой точки, определяющих ее траекторию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fb1c9-49ac-4146-afe0-1d3f4c66297d",
   "metadata": {},
   "source": [
    "6) 1-ая часть конструкции:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fc10e-eae1-47e7-b0cf-ae5af43fc448",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "    while True:\n",
    "        retur, current_frame = capture.read();\n",
    "        \n",
    "        if not retur:\n",
    "            break;\n",
    "\n",
    "        gray_map_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY);\n",
    "        new_points, point_status, error = cv2.calcOpticalFlowPyrLK(gray_map_initial, gray_map_current, points_to_track, None);\n",
    "\n",
    "        if point_status[0] == 1:\n",
    "            tracked_point = new_points[0][0];\n",
    "            trajectories.append((int(tracked_point[0]), int(tracked_point[1])));\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                source_points = np.array(trajectories[-3:], dtype = np.float32);\n",
    "                destination_points = np.array([points_to_track[0][0], tracked_point, tracked_point], dtype = np.float32);\n",
    "                M, inliers = cv2.estimateAffinePartial2D(source_points, destination_points);\n",
    "\n",
    "                if M is not None:\n",
    "                    points_to_track = cv2.transform(points_to_track, M);\n",
    "\n",
    "        ...\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c625c1-1ca5-45e3-80e8-f7b9a2cfe9f0",
   "metadata": {},
   "source": [
    "выполняет чтение каждого кадра видеопотока посредством функции \"capture.read()\". Текущий кадр переводится в цветовое пространство \"COLOR_BGR2GRAY\" посредством функции \"cv2.cvtColor()\". Затем, при помощи функции \"cv2.calcOpticalFlowPyrLK()\", вычисляется оптический поток между ключевыми точками текущего и предыдущего кадра в цветовом пространстве \"COLOR_BGR2GRAY\" при помощи итеративного метода Лукаса-Канады с использованием пирамид. Если ключевая точка в кадре найдена, то ее координаты звписываются в массив \"trajectories\". Когда в массиве \"trajectories\" будут находиться координаты как минимум 3-ех точек, то посредством функции \"cv2.estimateAffinePartial2D()\" будет вычисляться матрица афинного преобразования. Если матрица афинного преобразования определена, то при ее помощи вычисляется следующий набор ключевых точек для следующего кадра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105b683-60b4-4b36-966a-7a618e50169a",
   "metadata": {},
   "source": [
    "7) 2-ая часть конструкции:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f806905-5f04-4a70-873c-7c84bf8a293f",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "        ...\n",
    "    if len(trajectories) >= 3:\n",
    "                \n",
    "        for i in range(2, len(trajectories)):\n",
    "            cv2.line(current_frame, trajectories[i - 1], trajectories[i], (255, 0, 0), 2);\n",
    "\n",
    "    if len(trajectories) > 3:\n",
    "        cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "\n",
    "    cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "    cv2.drawMarker(current_frame, center_point, (0, 255, 0), markerType = cv2.MARKER_CROSS, markerSize = 20, thickness = 2);\n",
    "    end_point = (center_point[0] + 100, center_point[1] + 100);\n",
    "    cv2.line(first_frame, center_point, end_point, (255, 0, 0), 2);\n",
    "    draw_function(current_frame, tracked_point, trajectories, center_point);\n",
    "    points_to_track = new_points.reshape(-1, 1, 2);           \n",
    "        \n",
    "    else:\n",
    "        break;\n",
    "        \n",
    "    output.write(current_frame);\n",
    "    cv2.imshow(\"Video file point tracking in process...\", current_frame);\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break;\n",
    "\n",
    "    gray_map_initial = gray_map_current;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efd2d0-ea76-4ebe-8e64-a7d341cac75d",
   "metadata": {},
   "source": [
    "выполняет отрисовку опорной ключевой точки и точки, определяющей центр изображения, на каждом кадре, а также выполняет вызов функции \"draw_function()\". Затем выполняется вызов окна \"Video file point tracking in process...\" для каждого кадра видеопоследовательности. Конструкция \"cv2.waitKey(30) & 0xFF == ord('q')\" позволяет пользователю прервать цикл выполнения обработки видео посредством нажатия клавиши \"Q\", при этом уже обработанная часть видеопоследовательности будет сохранена в директорию проекта. Затем происходит замена изображений в цветовом пространстве \"COLOR_BGR2GRAY\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460f320-719f-4a38-aa22-82cac84a9ff9",
   "metadata": {},
   "source": [
    "Конструкция:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fb0a0-35d3-4f2c-8ce4-6aac4e38d788",
   "metadata": {},
   "source": [
    "    \"\"\"\n",
    "    capture.release();\n",
    "    output.release();\n",
    "    cv2.destroyAllWindows();\n",
    "    print(f\"Обработка видеопотока завершена. Обработанный файл {output_video} сохранен в директорию проекта\");\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad0b91-1a68-4fc1-988d-2ea81dd77746",
   "metadata": {},
   "source": [
    "выполняет процесс чтения входного видеопотока и записи выходного видеопотока;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8046939-e9f8-4e68-997c-a3ad98b633ef",
   "metadata": {},
   "source": [
    "Реализуем функцию \"draw_function()\", которая принимает на вход следующие аргументы:\n",
    "1) \"current_frame\" - текущий кадр видеопотока;\n",
    "2) \"tracked_point\" - ключевая точка, которая была определена;\n",
    "3) \"trajectories\" - массив, хранящий координаты ключевых точек;\n",
    "4) \"center_point\" - точка, определяющая центр каждого кадра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031a1a52-ad56-4e56-bb71-cfd8bdf4923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(current_frame, tracked_point, trajectories, center_point):\n",
    "    coordinates_text = f\"Координаты точки: X = {int(tracked_point[0])}, Y = {int(tracked_point[1])}\";\n",
    "    error_value = np.linalg.norm(np.array(trajectories[-1]) - np.array(tracked_point));\n",
    "    error_text = f\"Ошибка: {int(error_value)} пк\";   \n",
    "    cv2.putText(current_frame, coordinates_text, (20, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (100, 100, 100), 2);\n",
    "    cv2.putText(current_frame, error_text, (20, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (100, 100, 100), 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb546b2-b1b7-4ffb-b505-930cd269764d",
   "metadata": {},
   "source": [
    "Данная функция выполняет отрисовку текстовых данных о координатах опорной точки на каждом кадре видеопотока и ошибке его определения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc8a87-32e0-4419-9a45-6f6af7b2d2fc",
   "metadata": {},
   "source": [
    "Выполним вызов функции \"track_video_function()\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1f421be-f8f9-4f7c-a103-2d345bb394f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обработки видеопотока...\n",
      "Обработка видеопотока завершена. Обработанный файл Output_video_ORB.mp4 сохранен в директорию проекта\n"
     ]
    }
   ],
   "source": [
    "track_video_function('Input_video.mp4', 'Output_video_ORB.mp4');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8989ea-0591-47e6-8d4e-c30a0749cf44",
   "metadata": {},
   "source": [
    "Результат работы алгоритма с детектором \"ORB\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2940f5-c6a5-47ce-8816-6d194357c18c",
   "metadata": {},
   "source": [
    "<video controls src=\"Output_video_ORB.mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f8ffa-3490-4f5c-8900-394106b81fd3",
   "metadata": {},
   "source": [
    "Как можно увидеть, средняя ошибка определения ключевой точки составляет 1 пиксель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89498a08-e2c0-4c32-a75d-30abc4a8496b",
   "metadata": {},
   "source": [
    "Теперь реализуем обработку данного видео при помощи детектора \"SIFT\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac348bd6-6f9d-4be3-9831-4882b1eae6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_video_function(input_video, output_video):\n",
    "    print(\"Начало обработки видеопотока...\");\n",
    "    capture = cv2.VideoCapture(input_video);\n",
    "    video_fps = capture.get(cv2.CAP_PROP_FPS);\n",
    "    video_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH));\n",
    "    video_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT));\n",
    "    video_codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output = cv2.VideoWriter(output_video, video_codec, video_fps, (video_width, video_height));\n",
    "    retur, first_frame = capture.read();\n",
    "    \n",
    "    if not retur:\n",
    "        print(f\"Не удается прочитать видеофайл {input_video}\");\n",
    "        return;\n",
    "\n",
    "    detector_type = cv2.SIFT_create();\n",
    "    keypoints, descriptors = detector_type.detectAndCompute(first_frame, None);\n",
    "\n",
    "    if len(keypoints) > 0:\n",
    "        initial_point = keypoints[6700].pt;\n",
    "    \n",
    "    else:\n",
    "        print(\"Ключевые точки не найдены\");\n",
    "        return;\n",
    "\n",
    "    points_to_track = np.array([[initial_point]], dtype = np.float32);\n",
    "    gray_map_initial = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY);\n",
    "    center_point = (video_width // 2, video_height // 2);\n",
    "    output.write(first_frame);\n",
    "    trajectories = [];\n",
    "\n",
    "    while True:\n",
    "        retur, current_frame = capture.read();\n",
    "        \n",
    "        if not retur:\n",
    "            break;\n",
    "\n",
    "        gray_map_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY);\n",
    "        new_points, point_status, error = cv2.calcOpticalFlowPyrLK(gray_map_initial, gray_map_current, points_to_track, None);\n",
    "\n",
    "        if point_status[0] == 1:\n",
    "            tracked_point = new_points[0][0];\n",
    "            trajectories.append((int(tracked_point[0]), int(tracked_point[1])));\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                source_points = np.array(trajectories[-3:], dtype = np.float32);\n",
    "                destination_points = np.array([points_to_track[0][0], tracked_point, tracked_point], dtype = np.float32);\n",
    "                M, inliers = cv2.estimateAffinePartial2D(source_points, destination_points);\n",
    "\n",
    "                if M is not None:\n",
    "                    points_to_track = cv2.transform(points_to_track, M);\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                \n",
    "                for i in range(2, len(trajectories)):\n",
    "                    cv2.line(current_frame, trajectories[i - 1], trajectories[i], (255, 0, 0), 2);\n",
    "\n",
    "            if len(trajectories) > 3:\n",
    "                cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "\n",
    "            cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "            cv2.drawMarker(current_frame, center_point, (0, 255, 0), markerType = cv2.MARKER_CROSS, markerSize = 20, thickness = 2);\n",
    "            end_point = (center_point[0] + 100, center_point[1] + 100);\n",
    "            cv2.line(first_frame, center_point, end_point, (255, 0, 0), 2);\n",
    "            draw_function(current_frame, tracked_point, trajectories, center_point);\n",
    "            points_to_track = new_points.reshape(-1, 1, 2);           \n",
    "        \n",
    "        else:\n",
    "            break;  \n",
    "\n",
    "        output.write(current_frame);\n",
    "        cv2.imshow(\"Video file point tracking in process...\", current_frame);\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break;\n",
    "\n",
    "        gray_map_initial = gray_map_current;\n",
    "        \n",
    "    capture.release();\n",
    "    output.release();\n",
    "    cv2.destroyAllWindows();\n",
    "    print(f\"Обработка видеопотока завершена. Обработанный файл {output_video} сохранен в директорию проекта\");\n",
    "\n",
    "def draw_function(current_frame, tracked_point, trajectories, center_point):\n",
    "    coordinates_text = f\"Координаты точки: X = {int(tracked_point[0])}, Y = {int(tracked_point[1])}\";\n",
    "    error_value = np.linalg.norm(np.array(trajectories[-1]) - np.array(tracked_point));\n",
    "    error_text = f\"Ошибка: {int(error_value)} пк\";   \n",
    "    cv2.putText(current_frame, coordinates_text, (20, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (100, 100, 100), 2);\n",
    "    cv2.putText(current_frame, error_text, (20, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (100, 100, 100), 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b745d0-a307-408e-abb7-394c634f9bc6",
   "metadata": {},
   "source": [
    "Выполним обработку видеопотока при помощи детектора \"SIFT\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5789204-2bcf-4909-8972-fb7421359838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обработки видеопотока...\n",
      "Обработка видеопотока завершена. Обработанный файл Output_video_SIFT.mp4 сохранен в директорию проекта\n"
     ]
    }
   ],
   "source": [
    "track_video_function('Input_video.mp4', 'Output_video_SIFT.mp4');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e6636-a476-448f-9d64-6d33c009c94b",
   "metadata": {},
   "source": [
    "Результат работы алгоритма с детектором \"SIFT\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5ecda-db32-47fe-8959-366590184bbc",
   "metadata": {},
   "source": [
    "<video controls src=\"Output_video_SIFT.mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96641f-3c7f-4f56-84aa-8605a9b3445b",
   "metadata": {},
   "source": [
    "Здесь видно одну их основных проблем детектора \"SIFT\". Он имеет крайне большой дрейф ключевых точек при обработке кадров видеопотока, что описано в статье \"A Comparison of SIFT, SURF and ORB on OpenCV\" (https://mikhail-kennerley.medium.com/a-comparison-of-sift-surf-and-orb-on-opencv-59119b9ec3d0). Приведем иллюстрацию из данной статьи:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb655afb-a31c-4a75-aacc-f8ff512535cc",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1004/format:webp/1*tiR4AZSt-ltVlYv_3ds6Yw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab832798-c46a-4aa0-b19d-2923d2e605bc",
   "metadata": {},
   "source": [
    "Это приводит к тому, что опорная ключевая точка на объекте при его исчезновении из определенного кадра видеопотока плавно смещается в \"поле определения\" ближайшего объекта и алгоритм считает, что опорная точка не исчезла из поля зрения, что является проблемой. Также, по времени выполнения алгоритма, видно, что детектор \"SIFT\" работает медленнее детектора \"ORB\", что также проиллюстрированно в статье \"A Comparison of SIFT, SURF and ORB on OpenCV\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df973914-4734-412f-876d-82d51f476621",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1010/format:webp/1*cf44qgjFB-tjNiG1Nn5n7A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a2718-24ec-4a08-8865-5ba84011d3f2",
   "metadata": {},
   "source": [
    "В связи с вышеперечисленными недостатками детектора \"SIFT\", в дальнейшем мы будем использовать детектор \"ORB\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddf8fc-b19f-4d02-9e20-1ce9efcc5182",
   "metadata": {},
   "source": [
    "## 2. Добавление фильтра шумов в видеопоток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3d3bf-cd93-4836-a351-fb7246d5fd4f",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию \"distortion_noise_function()\", которая принимает на вход следующие аргументы:\n",
    "1) \"frame\" - текущий кадр;\n",
    "2) \"distortion_level\" - уровень шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3954d52-87e3-4f27-a6f5-d8d7369aec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_noise_function(frame, distortion_level=0.65):\n",
    "    noise = np.random.normal(0, 25, frame.shape).astype(np.uint8);\n",
    "    distorted_frame = cv2.addWeighted(frame, 1 - distortion_level, noise, distortion_level, 0);\n",
    "    return distorted_frame;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5492057-97a1-4537-9120-359b3d5e69ef",
   "metadata": {},
   "source": [
    "Данная функция генерирует случайный шум. Шум создается с использованием нормального распределения с матожиданием \"0\" и стандартным отклонением \"25\", что приводит к случайным значениям яркости, варьирующимся вдоль пикселей. Результирующий массив преобразуется в тип \"uint8\", что соответствует типу данных, используемому для представления пикселей изображений в \"OpenCV\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865019b-edd8-4ba4-b69c-f45031ee548c",
   "metadata": {},
   "source": [
    "Теперь модифицируем код, добавив вычисление отклонения координат опорной ключевой точки между кадром без искажений и кадром с добавлением случайного шума вдоль осей \"X\" и \"Y\". Также добавим отображение данных отклонений на каждый кадр видеопотока:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3e9cbc-b02a-48d4-9ce3-8a04c33662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок импорта библиотек\n",
    "\n",
    "import cv2;\n",
    "import numpy as np;\n",
    "\n",
    "# Блок функций\n",
    "\n",
    "def track_video_function(input_video, output_video):\n",
    "    print(\"Начало обработки видеопотока...\");\n",
    "    capture = cv2.VideoCapture(input_video);\n",
    "    video_fps = capture.get(cv2.CAP_PROP_FPS);\n",
    "    video_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH));\n",
    "    video_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT));\n",
    "    video_codec = cv2.VideoWriter_fourcc(*'mp4v');\n",
    "    output = cv2.VideoWriter(output_video, video_codec, video_fps, (video_width, video_height));\n",
    "    retur, first_frame = capture.read();\n",
    "    first_frame = distortion_noise_function(first_frame);\n",
    "    \n",
    "    if not retur:\n",
    "        print(f\"Не удается прочитать видеофайл {input_video}\");\n",
    "        return;\n",
    "\n",
    "    detector_type = cv2.ORB_create();\n",
    "    keypoints, descriptors = detector_type.detectAndCompute(first_frame, None);\n",
    "\n",
    "    if len(keypoints) > 0:\n",
    "        initial_point = keypoints[240].pt;\n",
    "    else:\n",
    "        print(\"Ключевые точки не найдены\");\n",
    "        return;\n",
    "\n",
    "    points_to_track = np.array([[initial_point]], dtype=np.float32);\n",
    "    gray_map_initial = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY);\n",
    "    center_point = (video_width // 2, video_height // 2);\n",
    "    output.write(first_frame);\n",
    "    trajectories = [];\n",
    "    original_point = points_to_track[0][0];\n",
    "    total_error_x = 0;\n",
    "    total_error_y = 0;\n",
    "    frame_count = 0;\n",
    "\n",
    "    while True:\n",
    "        retur, current_frame = capture.read();\n",
    "        current_frame = distortion_noise_function(current_frame);\n",
    "        \n",
    "        if not retur:\n",
    "            break;\n",
    "        \n",
    "        gray_map_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY);\n",
    "        new_points, point_status, error = cv2.calcOpticalFlowPyrLK(gray_map_initial, gray_map_current, points_to_track, None);\n",
    "\n",
    "        if point_status[0] == 1:\n",
    "            tracked_point = new_points[0][0];\n",
    "            trajectories.append((int(tracked_point[0]), int(tracked_point[1])));\n",
    "            difference = np.array(tracked_point) - np.array(original_point);\n",
    "            original_point = tracked_point;\n",
    "            error_value_x = difference[0];\n",
    "            error_value_y = difference[1];\n",
    "            total_error_x += error_value_x;\n",
    "            total_error_y += error_value_y;\n",
    "            frame_count += 1;\n",
    "            \n",
    "            if len(trajectories) >= 3:\n",
    "                source_points = np.array(trajectories[-3:], dtype=np.float32);\n",
    "                destination_points = np.array([points_to_track[0][0], tracked_point, tracked_point], dtype=np.float32);\n",
    "                M, inliers = cv2.estimateAffinePartial2D(source_points, destination_points);\n",
    "\n",
    "                if M is not None:\n",
    "                    points_to_track = cv2.transform(points_to_track, M);\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                for i in range(2, len(trajectories)):\n",
    "                    cv2.line(current_frame, trajectories[i - 1], trajectories[i], (255, 0, 0), 2);\n",
    "\n",
    "            if len(trajectories) > 3:\n",
    "                cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "\n",
    "            cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "            cv2.drawMarker(current_frame, center_point, (0, 255, 0), markerType=cv2.MARKER_CROSS, markerSize=20, thickness=2);\n",
    "            end_point = (center_point[0] + 100, center_point[1] + 100);\n",
    "            cv2.line(first_frame, center_point, end_point, (255, 0, 0), 2);\n",
    "            draw_function(current_frame, tracked_point, trajectories, center_point, difference, total_error_x, total_error_y, frame_count);\n",
    "            points_to_track = new_points.reshape(-1, 1, 2);\n",
    "            \n",
    "        else:\n",
    "            break;\n",
    "\n",
    "        output.write(current_frame);\n",
    "        cv2.imshow(\"Video file point tracking in process...\", current_frame);\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break;\n",
    "\n",
    "        gray_map_initial = gray_map_current;\n",
    "\n",
    "    capture.release();\n",
    "    output.release();\n",
    "    cv2.destroyAllWindows();\n",
    "    print(f\"Обработка видеопотока завершена. Обработанный файл {output_video} сохранен в директорию проекта\");\n",
    "\n",
    "def draw_function(current_frame, tracked_point, trajectories, center_point, difference, total_error_x, total_error_y, frame_count):\n",
    "    coordinates_text = f\"Координаты точки: X = {int(tracked_point[0])}, Y = {int(tracked_point[1])}\";\n",
    "    error_value = np.linalg.norm(np.array(trajectories[-1]) - np.array(tracked_point));\n",
    "    error_text = f\"Ошибка: {int(error_value)} пк\";\n",
    "    difference_text = f\"Смещение опорной ключевой точки с учетом шумов: dX = {int(difference[0])}, dY = {int(difference[1])}\";\n",
    "    average_error_x_text = f\"Средняя ошибка смещения с учетом шума: dX = {float(total_error_x / frame_count) if frame_count > 0 else 0} пк\";\n",
    "    average_error_y_text = f\"Средняя ошибка смещения с учетом шума: dY = {float(total_error_y / frame_count) if frame_count > 0 else 0} пк\";\n",
    "    cv2.putText(current_frame, coordinates_text, (20, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    cv2.putText(current_frame, error_text, (20, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    cv2.putText(current_frame, difference_text, (20, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    cv2.putText(current_frame, average_error_x_text, (20, 160), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    cv2.putText(current_frame, average_error_y_text, (20, 200), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf208ce6-6520-494d-b537-07392eb0da89",
   "metadata": {},
   "source": [
    "Теперь выполним вычисление отклонения координат опорной ключевой точки между кадром без искажений и кадром с добавлением случайного шума, c уровнем \"0.65\", вдоль осей \"X\" и \"Y\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e9c8f6b-2205-43c0-ac39-fcf85a7fda7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обработки видеопотока...\n",
      "Обработка видеопотока завершена. Обработанный файл Output_video_(noise).mp4 сохранен в директорию проекта\n"
     ]
    }
   ],
   "source": [
    "track_video_function('Input_video.mp4', 'Output_video_(noise).mp4');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1838fe7-21ae-41d0-98c6-0c883434195f",
   "metadata": {},
   "source": [
    "Результат обработки видеопотока с шумами:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c70db-eefa-4cd2-88e5-37685a374894",
   "metadata": {},
   "source": [
    "<video controls src=\"Output_video_(noise).mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfcb1c-5150-44e7-8256-6c4651a9cef0",
   "metadata": {},
   "source": [
    "По результатам обработки получены следующие результаты:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5b27f-e16f-4b96-a888-e6798fb5b4cc",
   "metadata": {},
   "source": [
    "dX = -0.71 пикселя;\n",
    "dY = 1.42 пикселя."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f673a-d42e-4ada-b1ac-e9842e7f5ff8",
   "metadata": {},
   "source": [
    "Это средняя величина ошибки координат опорной ключевой точки между кадром без искажений и кадром с добавлением случайного шума, c уровнем \"0.65\", вдоль осей \"X\" и \"Y\" c учетом направления смещения относительно номинального положения (с учетом знака). Абсолютная средняя величина ошибки будет больше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b58fa-8539-460e-8ab4-563ee64ed755",
   "metadata": {},
   "source": [
    "## 3. Добавление фильтра резкости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90773986-11c5-46e3-86af-d2171585ebdf",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию \"sobel_filter_function()\", которая принимает на вход следующие аргументы:\n",
    "1) \"frame\" - текущий кадр;\n",
    "2) \"alpha\" - коэффициент перекрытия текущего кадра фильтром."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68345a2a-53f8-4f34-9dc7-4b8f8dcdfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_filter_function(frame, alpha=1.5):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]]);\n",
    "    filter_add = cv2.filter2D(frame, -1, kernel);\n",
    "    filtered_frame = cv2.addWeighted(frame, 1 - alpha, filter_add, alpha, 0);\n",
    "    return filtered_frame;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c2e7d-755c-41c2-83d2-43caa3036edb",
   "metadata": {},
   "source": [
    "В данной функции создается матрица (ядро) \"kernel\" для фильтра Собеля, предназначенная для увеличения резкости изображения. \n",
    "Центральный элемент матрицы повышает яркость пикселя, в то время как соседние элементы уменьшают её, что выделяет края объектов на изображении. Затем данный фильтр применяется к текущему кадру при помощи функций \"cv2.filter2D()\", \"cv2.addWeighted()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b7fe2-6b1a-4cd4-b76d-4b764079b176",
   "metadata": {},
   "source": [
    "Теперь модифицируем код, добавив фильтр резкости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "26411c6e-123d-4317-a7e0-eac7fd881cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок импорта библиотек\n",
    "\n",
    "import cv2;\n",
    "import numpy as np;\n",
    "\n",
    "# Блок функций\n",
    "\n",
    "def track_video_function(input_video, output_video):\n",
    "    print(\"Начало обработки видеопотока...\");\n",
    "    capture = cv2.VideoCapture(input_video);\n",
    "    video_fps = capture.get(cv2.CAP_PROP_FPS);\n",
    "    video_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH));\n",
    "    video_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT));\n",
    "    video_codec = cv2.VideoWriter_fourcc(*'mp4v');\n",
    "    output = cv2.VideoWriter(output_video, video_codec, video_fps, (video_width, video_height));\n",
    "    retur, first_frame = capture.read();\n",
    "    first_frame = sobel_filter_function(first_frame);\n",
    "    \n",
    "    if not retur:\n",
    "        print(f\"Не удается прочитать видеофайл {input_video}\");\n",
    "        return;\n",
    "\n",
    "    detector_type = cv2.ORB_create();\n",
    "    keypoints, descriptors = detector_type.detectAndCompute(first_frame, None);\n",
    "\n",
    "    if len(keypoints) > 0:\n",
    "        initial_point = keypoints[360].pt;\n",
    "    else:\n",
    "        print(\"Ключевые точки не найдены\");\n",
    "        return;\n",
    "\n",
    "    points_to_track = np.array([[initial_point]], dtype=np.float32);\n",
    "    gray_map_initial = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY);\n",
    "    center_point = (video_width // 2, video_height // 2);\n",
    "    output.write(first_frame);\n",
    "    trajectories = [];\n",
    "\n",
    "    while True:\n",
    "        retur, current_frame = capture.read();\n",
    "        current_frame = sobel_filter_function(current_frame);\n",
    "        \n",
    "        if not retur:\n",
    "            break;\n",
    "        \n",
    "        gray_map_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY);\n",
    "        new_points, point_status, error = cv2.calcOpticalFlowPyrLK(gray_map_initial, gray_map_current, points_to_track, None);\n",
    "\n",
    "        if point_status[0] == 1:\n",
    "            tracked_point = new_points[0][0];\n",
    "            trajectories.append((int(tracked_point[0]), int(tracked_point[1])));\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                source_points = np.array(trajectories[-3:], dtype=np.float32);\n",
    "                destination_points = np.array([points_to_track[0][0], tracked_point, tracked_point], dtype=np.float32);\n",
    "                M, inliers = cv2.estimateAffinePartial2D(source_points, destination_points);\n",
    "\n",
    "                if M is not None:\n",
    "                    points_to_track = cv2.transform(points_to_track, M);\n",
    "\n",
    "            if len(trajectories) >= 3:\n",
    "                for i in range(2, len(trajectories)):\n",
    "                    cv2.line(current_frame, trajectories[i - 1], trajectories[i], (255, 0, 0), 2);\n",
    "\n",
    "            if len(trajectories) > 3:\n",
    "                cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "\n",
    "            cv2.circle(current_frame, (int(tracked_point[0]), int(tracked_point[1])), 10, (0, 0, 255), -1);\n",
    "            cv2.drawMarker(current_frame, center_point, (0, 255, 0), markerType=cv2.MARKER_CROSS, markerSize=20, thickness=2);\n",
    "            end_point = (center_point[0] + 100, center_point[1] + 100);\n",
    "            cv2.line(first_frame, center_point, end_point, (255, 0, 0), 2);\n",
    "            draw_function(current_frame, tracked_point, trajectories, center_point);\n",
    "            points_to_track = new_points.reshape(-1, 1, 2);\n",
    "            \n",
    "        else:\n",
    "            break;\n",
    "\n",
    "        output.write(current_frame);\n",
    "        cv2.imshow(\"Video file point tracking in process...\", current_frame);\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break;\n",
    "\n",
    "        gray_map_initial = gray_map_current;\n",
    "\n",
    "    capture.release();\n",
    "    output.release();\n",
    "    cv2.destroyAllWindows();\n",
    "    print(f\"Обработка видеопотока завершена. Обработанный файл {output_video} сохранен в директорию проекта\");\n",
    "\n",
    "def draw_function(current_frame, tracked_point, trajectories, center_point):\n",
    "    coordinates_text = f\"Координаты точки: X = {int(tracked_point[0])}, Y = {int(tracked_point[1])}\";\n",
    "    error_value = np.linalg.norm(np.array(trajectories[-1]) - np.array(tracked_point));\n",
    "    error_text = f\"Ошибка: {int(error_value)} пк\";\n",
    "    cv2.putText(current_frame, coordinates_text, (20, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    cv2.putText(current_frame, error_text, (20, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2);\n",
    "    \n",
    "def sobel_filter_function(frame, alpha=1.5):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]]);\n",
    "    filter_add = cv2.filter2D(frame, -1, kernel);\n",
    "    filtered_frame = cv2.addWeighted(frame, 1 - alpha, filter_add, alpha, 0);\n",
    "    return filtered_frame;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb37634-db71-405c-8adc-753011403d49",
   "metadata": {},
   "source": [
    "Выполним обработку изображения применив фильтр резкости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8f4075c-524f-4918-acee-07857f6a9fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обработки видеопотока...\n",
      "Обработка видеопотока завершена. Обработанный файл Output_video_(sobel_filter).mp4 сохранен в директорию проекта\n"
     ]
    }
   ],
   "source": [
    "track_video_function('Input_video.mp4', 'Output_video_(sobel_filter).mp4');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ccaa9-67ae-460c-b467-ff35d175fdb5",
   "metadata": {},
   "source": [
    "Результат обработки видеопотока с фильтром резкости:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf6d6a-8fb0-46e1-bd95-aa8fbceb8812",
   "metadata": {},
   "source": [
    "<video controls src=\"Output_video_(sobel_filter).mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c336246-f2ac-4ce7-8a0f-8138335050f0",
   "metadata": {},
   "source": [
    "## Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0a3f9-66d0-4cec-8f89-e49792534cbb",
   "metadata": {},
   "source": [
    "В результате работы были выполнены все цели и задачи. Было установлено, что применение детектора \"ORB\" для видеопотока предпочтительнее, чем детектора \"SIFT\", поскольку детектор \"ORB\" имеет лучшее быстродействие, а также почти не подвержен дрейфу ключевых точек. Также было установлено, что применение фильтра шумов к видеопотоку ухудшает точность определения координат ключевых точек для каждого кадра."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
