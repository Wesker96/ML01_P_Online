{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "Для любых пар изображений которые имеют общее поле зрения (две фотографии сделанные с телефона который был повернут на определенный угол - любой лишь бы было 20+% перекрытия)\n",
    "- Рассчитать матрицу афинного преобразования из одной системы координат в другую.\n",
    "- Выполнить разложение данной матрицы и посчитать явно угол поворота, вектор переноса и скалирования. \n",
    "- Выполнить обратное преобразование - получить афинную матрицу заново. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ\n",
    "Для наших эксперементов подготовим сцену в программе Blender. Камера всегда будет направлена в надир.\n",
    "Подготовим 2 кадра местности. 2 кадр будет повернут на 30 градусов и немного приближен. Качество найденной матрицы изучим визуальным путем наложив 1 изображение на 2 используя матрицу гомографии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Сцена в Blender](./assets/section1/blender.png)\n",
    "- [Основной кадр](./assets/section1/1.png)\n",
    "- [Кадр для сравнения](./assets/section1/2.png)\n",
    "\n",
    "Код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches before filtering: 44680\n",
      "Number of matches after filtering: 22785\n",
      "Матрица гомографии: \n",
      "[[ 8.66020317e-01 -5.00002098e-01  5.31329760e+02]\n",
      " [ 5.00002399e-01  8.66034014e-01 -5.43454992e+02]\n",
      " [-3.09005308e-09  6.45127710e-09  1.00000000e+00]]\n",
      "Угол: 30.000061086982882\n",
      "Коэффициент увеличения: 1.414217309710615\n",
      "Перевод: [[ 531.32975972]\n",
      " [-543.45499202]]\n",
      "Перекомпонованная матрица: \n",
      "[[ 8.66033098e-01 -4.99998823e-01  5.31329760e+02]\n",
      " [ 5.00005673e-01  8.66021233e-01 -5.43454992e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      "Равны ли матрица гомографии и перекомпонованная матрица? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(image, detector='sift', mask=None):\n",
    "    if detector == 'sift':\n",
    "        det = cv2.SIFT_create()\n",
    "    elif detector == 'orb':\n",
    "        det = cv2.ORB_create()\n",
    "    elif detector == 'kaze':\n",
    "        det = cv2.KAZE_create()\n",
    "    elif detector == 'akaze':\n",
    "        det = cv2.AKAZE_create()\n",
    "    elif detector == 'brisk':\n",
    "        det = cv2.BRISK_create()\n",
    "        \n",
    "    kp, des = det.detectAndCompute(image, mask)\n",
    "    \n",
    "    return kp, des\n",
    "\n",
    "def match_features(des1, des2, matching='BF', detector='sift', sort=True, k=2):\n",
    "    if matching == 'BF':\n",
    "        matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck=False)\n",
    "        if detector == 'orb':\n",
    "            matcher = cv2.BFMatcher_create(cv2.NORM_HAMMING2, crossCheck=False)\n",
    "        matches = matcher.knnMatch(des1, des2, k=k)\n",
    "    elif matching == 'FLANN':\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = matcher.knnMatch(des1, des2, k=k)\n",
    "    \n",
    "    if sort:\n",
    "        matches = sorted(matches, key = lambda x:x[0].distance)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def filter_matches_distance(matches, dist_threshold):\n",
    "    filtered_match = []\n",
    "    for m, n in matches:\n",
    "        if m.distance <= dist_threshold*n.distance:\n",
    "            filtered_match.append(m)\n",
    "\n",
    "    return filtered_match\n",
    "\n",
    "def decompose_affine_matrix(affine):\n",
    "    if affine.shape != (3, 3) or affine.dtype != np.float64:\n",
    "        raise ValueError(\"Invalid input matrix. Must be a 3x3 double matrix.\")\n",
    "\n",
    "    R = affine[:2, :2]\n",
    "    U, W, Vt = np.linalg.svd(R)\n",
    "\n",
    "    rotation = np.dot(U, Vt)\n",
    "    scaling = np.diag(W)\n",
    "    translation = affine[:2, 2:]\n",
    "\n",
    "    return rotation, translation, scaling\n",
    "\n",
    "def recompose_affine_matrix(rotation, translation, scaling):\n",
    "    rotation_scaling = np.dot(rotation, scaling)\n",
    "    \n",
    "    affine = np.eye(3)\n",
    "    affine[:2, :2] = rotation_scaling\n",
    "    affine[:2, 2] = translation.flatten()\n",
    "    \n",
    "    return affine\n",
    "\n",
    "def get_angle(rotation):\n",
    "    return np.arctan2(rotation[1, 0], rotation[0, 0])\n",
    "\n",
    "def get_scale(scaling):\n",
    "    return np.sqrt(scaling[0, 0] ** 2 + scaling[1, 1] ** 2)\n",
    "\n",
    "def get_translation(translation):\n",
    "    return translation\n",
    "\n",
    "def get_angle_degrees(rotation):\n",
    "    return np.degrees(get_angle(rotation))\n",
    "\n",
    "img0 = cv2.imread(\"./assets/section1/1.png\")\n",
    "img1 = cv2.imread(\"./assets/section1/2.png\")\n",
    "\n",
    "kp0, des0 = extract_features(img0, 'sift')\n",
    "kp1, des1 = extract_features(img1, 'sift')\n",
    "\n",
    "matches = match_features(des0, des1, matching='BF', detector='sift', sort=True)\n",
    "print('Number of matches before filtering:', len(matches))\n",
    "matches = filter_matches_distance(matches, 0.75)\n",
    "print('Number of matches after filtering:', len(matches))\n",
    "\n",
    "src_pts = np.float32([kp0[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([kp1[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "H, _ = cv2.findHomography(src_pts, dst_pts, cv2.LMEDS)\n",
    "\n",
    "print(f\"Матрица гомографии: \\n{H}\")\n",
    "\n",
    "rotation, translation, scaling = decompose_affine_matrix(H)\n",
    "\n",
    "print(f\"Угол: {get_angle_degrees(rotation)}\")\n",
    "print(f\"Коэффициент увеличения: {get_scale(scaling)}\")\n",
    "print(f\"Перевод: {get_translation(translation)}\")\n",
    "\n",
    "recomposed_affine = recompose_affine_matrix(rotation, translation, scaling)\n",
    "\n",
    "print(f\"Перекомпонованная матрица: \\n{recomposed_affine}\")\n",
    "\n",
    "is_close = np.allclose(H, recomposed_affine, rtol=1e-04)\n",
    "print(f\"\\nРавны ли матрица гомографии и перекомпонованная матрица? {is_close}\")\n",
    "\n",
    "# Наложим 1 изображение на 2 для проверки правильности найденной матрицы гомографии\n",
    "height, width, _ = img0.shape\n",
    "warped_image1 = cv2.warpPerspective(img1, np.linalg.inv(H), (width, height))\n",
    "combined_image = cv2.addWeighted(warped_image1, 0.5, img0, 0.5, 0)\n",
    "cv2.imwrite(\"./assets/section1/warped_image.png\", combined_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наложим одно изображение на другое:\n",
    "\n",
    "\n",
    "![Сцена в Blender](./assets/section1/warped_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы 1 задания\n",
    "- Матрица гомографии найдена достоточно точно. Это можно проверить визуально по наложенной картинке и по найденному углу 30.00006 (задан 30)\n",
    "- Удалось получить начальную матрицу гомографии после ее декомпозиции (с учетом погрешности вычислений)\n",
    "- Изучиил базовые принципы поиска и использования ключевых точек для поиска матрицы гомографии, сопоставления картинок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "- Взять любую видеопоследовательность (желательно вид сверху) - например полет дрона. Можете скачать из  интернета или взять что то с работы. \n",
    "- Зафиксировав точку (например центр экрана) выполнить стабилизацию данной точки с использованием матрицы афинного преобразования \n",
    "(считая матрицу между кадрами вы всегда сможете понять где именно находится точка на изображении, а по отклонению самой точки вы можете определить точность самого алгоритма).\n",
    "- Не берите 4к - долго считать, HD будет достаточно\n",
    "- Посчитать расстояние между точками и ошибку в пикселях.\n",
    "- Попробуйте добавить шум к изображению, посмотрите как меняется точность (положение точки). \n",
    "- Добавьте фильтр к изображению для улучшения контрастности (любой который найдете).\n",
    "- Оцените как влияет выбор метода определения матрицы афинного преобразования - например estimateAffine2D и его аналоги (аналоги найдите самии).\n",
    "- Оцените как влияет выбор детектора/дескриптора особых точек на результат. Какой метод даст лучший и самый быстрый результат? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ\n",
    "- Многие функции будем просто переиспользовать из 1 задания (требуется запускать ячейки последовательно).\n",
    "- Отрендерим анимацию движения в 1 сторону и возврат в начальное положения. На видео добавим все варианты движения камеры (поворот, зум, перевод). В таком случае мы легко сможем найти ошибку т.к. мы точно знаем, что после преобразований точка должна вернуться к начальному положению.\n",
    "- В качестве фильтра для улучшения контрастности будем использовать локальное повышение контрастности CLAHE.    \n",
    "- Будем искать матрицу гомографии между 2 соседними кадрами видео и накапливать трансформации в H_accumulated.  \n",
    "- Для визуального контроля правильности работы алгоритма будем строить карту. Для этого создадим холст размера которого нам хватит с запасом и используя матрицу гомографии и функцию warpPerspective будем налаживать по маске холст и каждый кадр.\n",
    "- В качестве генератора шума будем использовать Гаусов шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './assets/section2/0001-0480.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    date            : 2024/12/04 08:20:55\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 30083 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 2560x1440 [SAR 1:1 DAR 16:9], 30080 kb/s, 24 fps, 24 tbr, 12288 tbn, 48 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> fps\n",
      "  paletteuse -> Stream #0:0 (gif)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;32m[Parsed_palettegen_3 @ 0x63de6abf1e80] \u001b[0m255(+1) colors generated out of 122750 colors; ratio=0.002077\n",
      "Output #0, gif, to './assets/section2/0001-0480_compressed.gif':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    date            : 2024/12/04 08:20:55\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: gif, pal8(pc, gbr/unknown/unknown, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 12 fps, 100 tbn (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 gif\n",
      "frame=  240 fps= 62 q=-0.0 Lsize=   56242kB time=00:00:19.93 bitrate=23117.7kbits/s speed=5.18x     \n",
      "video:56242kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000035%\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i ./assets/section2/0001-0480.mp4 -filter_complex \"fps=12, scale=-1:360[s]; [s]split[a][b]; [a]palettegen[palette]; [b][palette]paletteuse\" ./assets/section2/0001-0480_compressed.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовое видео (сжатое):\n",
    "\n",
    "![Сцена в Blender](./assets/section2/0001-0480_compressed.gif )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import time\n",
    "\n",
    "def local_contrast_increase(frame):\n",
    "    clahe = cv2.createCLAHE(clipLimit=5., tileGridSize=(8,8))\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l2 = clahe.apply(l)\n",
    "    lab = cv2.merge((l2,a,b))\n",
    "    \n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def get_affine_matrix(matrix_type, src_pts, dst_pts, matrix_estimate_method):\n",
    "    method = cv2.RANSAC\n",
    "\n",
    "    if matrix_estimate_method == 'rho':\n",
    "        method = cv2.RHO\n",
    "    elif matrix_estimate_method == 'lmeds':\n",
    "        method = cv2.LMEDS\n",
    "\n",
    "    if matrix_type == 'findHomography':\n",
    "        return cv2.findHomography(src_pts, dst_pts, method)\n",
    "    elif matrix_type == 'estimateAffine2D':\n",
    "        matrix, mask = cv2.estimateAffine2D(src_pts, dst_pts, method)\n",
    "        return (np.vstack([matrix, [0, 0, 1]]), mask)\n",
    "    elif matrix_type == 'estimateAffinePartial2D':\n",
    "        matrix, mask = cv2.estimateAffinePartial2D(src_pts, dst_pts, method)\n",
    "        return (np.vstack([matrix, [0, 0, 1]]), mask)\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, var=0.01):\n",
    "    image = np.array(image, dtype=np.float32) / 255.0\n",
    "    noise = np.random.normal(mean, var ** 0.5, image.shape)\n",
    "    noisy_image = image + noise\n",
    "    noisy_image = np.clip(noisy_image, 0, 1)\n",
    "    noisy_image = np.uint8(noisy_image * 255)\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "def homography_experenent(use_local_contrast_filter, detector, matching, matrix_type, matrix_estimate_method, use_gaussian_noise, video_path = './assets/section2/0001-0480.mp4'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    is_first_frame = True\n",
    "    output_map_size = (frame_width // 2, frame_height)\n",
    "    builded_map = np.zeros((output_map_size[1], output_map_size[0], 3), dtype=np.uint8) # Изменено для поддержки цветного изображения\n",
    "\n",
    "    # Начальная матрица гомографии\n",
    "    H_accumulated = np.array([[1, 0, frame_width // 8],\n",
    "                            [0, 1, frame_height // 8],\n",
    "                            [0, 0, 1]], dtype=np.float32)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.resize(frame, (frame_width // 4, frame_height // 4))\n",
    "        \n",
    "        if use_local_contrast_filter:\n",
    "            frame = local_contrast_increase(frame)\n",
    "        \n",
    "        if use_gaussian_noise:\n",
    "            frame = add_gaussian_noise(frame)\n",
    "\n",
    "        if is_first_frame:\n",
    "            is_first_frame = False\n",
    "            kp0, des0 = extract_features(frame, detector)\n",
    "            continue\n",
    "        \n",
    "        kp1, des1 = extract_features(frame, detector)\n",
    "        \n",
    "        try:\n",
    "            matches = match_features(des0, des1, matching=matching, detector=detector, sort=True)\n",
    "            matches = filter_matches_distance(matches, 0.75)\n",
    "            src_pts = np.float32([kp0[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([kp1[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            H, _ = get_affine_matrix(matrix_type, src_pts, dst_pts, matrix_estimate_method)\n",
    "            H_accumulated = H_accumulated @ np.linalg.inv(H)\n",
    "            kp0, des0 = kp1, des1\n",
    "            warped_img = cv2.warpPerspective(frame, H_accumulated, output_map_size, borderMode=cv2.BORDER_TRANSPARENT) # BORDER_CONSTANT\n",
    "            mask = np.zeros_like(builded_map, dtype=np.uint8)\n",
    "            mask[warped_img > 0] = 1\n",
    "            builded_map = cv2.add(builded_map * (1 - mask), warped_img * mask)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    \n",
    "    if not use_gaussian_noise:\n",
    "        cv2.imwrite(f\"./assets/section2/builded_maps/builded_map_{detector}_{matching}_filtered-{use_local_contrast_filter}_{matrix_type}_{matrix_estimate_method}.png\", builded_map)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return math.sqrt((H_accumulated[0, 2] - (frame_width // 8)) ** 2 + (H_accumulated[1, 2] - (frame_height // 8)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор лучших гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры 20 лучших вариантов:\n",
      "Ошибка: 0.2765 px, Params: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'ransac', 40.382333278656006)\n",
      "Ошибка: 0.2765 px, Params: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'rho', 40.39733123779297)\n",
      "Ошибка: 0.2765 px, Params: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'lmeds', 39.792399644851685)\n",
      "Ошибка: 0.3489 px, Params: (True, 'kaze', 'FLANN', 'estimateAffine2D', 'lmeds', 70.56719732284546)\n",
      "Ошибка: 0.3565 px, Params: (True, 'kaze', 'FLANN', 'estimateAffine2D', 'rho', 69.8750958442688)\n",
      "Ошибка: 0.4419 px, Params: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'lmeds', 45.66717267036438)\n",
      "Ошибка: 0.4613 px, Params: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'ransac', 63.11133646965027)\n",
      "Ошибка: 0.4898 px, Params: (False, 'kaze', 'FLANN', 'estimateAffine2D', 'ransac', 46.34456396102905)\n",
      "Ошибка: 0.5215 px, Params: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'ransac', 45.411500692367554)\n",
      "Ошибка: 0.5504 px, Params: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'lmeds', 64.30302786827087)\n",
      "Ошибка: 0.5597 px, Params: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'rho', 46.87816667556763)\n",
      "Ошибка: 0.5932 px, Params: (False, 'kaze', 'FLANN', 'estimateAffine2D', 'lmeds', 46.53315997123718)\n",
      "Ошибка: 0.7419 px, Params: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'ransac', 28.56929326057434)\n",
      "Ошибка: 0.7419 px, Params: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'rho', 28.332597494125366)\n",
      "Ошибка: 0.7419 px, Params: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'lmeds', 28.479191064834595)\n",
      "Ошибка: 0.7820 px, Params: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'ransac', 16.058214902877808)\n",
      "Ошибка: 0.7820 px, Params: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'rho', 16.333191633224487)\n",
      "Ошибка: 0.7820 px, Params: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'lmeds', 16.142710208892822)\n",
      "Ошибка: 1.0424 px, Params: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'rho', 63.29391074180603)\n",
      "Ошибка: 1.0763 px, Params: (True, 'sift', 'FLANN', 'findHomography', 'rho', 63.487687826156616)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for detector in ['sift', 'orb', 'kaze', 'akaze', 'brisk']:\n",
    "    for matching in ['BF', 'FLANN']: \n",
    "        for matrix_type in ['estimateAffine2D','findHomography', 'estimateAffinePartial2D']:\n",
    "            for matrix_estimate_method in ['ransac', 'rho', 'lmeds']:\n",
    "                for use_local_contrast_filter in [True, False]:\n",
    "                    start_time = time.time()\n",
    "                    error = homography_experenent(use_local_contrast_filter, detector, matching, matrix_type, matrix_estimate_method, False)\n",
    "                    results.append((error, use_local_contrast_filter, detector, matching, matrix_type, matrix_estimate_method, time.time() - start_time))\n",
    "results.sort(key=lambda x: x[0])\n",
    "\n",
    "print('Параметры 20 лучших вариантов:')\n",
    "for result in results[:20]:\n",
    "    print(f\"Ошибка: {result[0]:.4f} px, Params: {result[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Влияние шума на качество построения карты. Используем 20 лучших подобранных параметров и посмотрим на сколько изменилась ошибка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'ransac'). Изменение ошибки: 0.464865668529866 px\n",
      "Параметры: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'rho'). Изменение ошибки: 2.0499276667798023 px\n",
      "Параметры: (True, 'sift', 'BF', 'estimateAffinePartial2D', 'lmeds'). Изменение ошибки: 0.9856152242130536 px\n",
      "Параметры: (True, 'kaze', 'FLANN', 'estimateAffine2D', 'lmeds'). Изменение ошибки: 3.580521554536231 px\n",
      "Параметры: (True, 'kaze', 'FLANN', 'estimateAffine2D', 'rho'). Изменение ошибки: 4.577513129107985 px\n",
      "Параметры: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'lmeds'). Изменение ошибки: 4.4092174867609355 px\n",
      "Параметры: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'ransac'). Изменение ошибки: -0.15477936254897773 px\n",
      "Параметры: (False, 'kaze', 'FLANN', 'estimateAffine2D', 'ransac'). Изменение ошибки: 4.342403125137536 px\n",
      "Параметры: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'ransac'). Изменение ошибки: 5.618082757992347 px\n",
      "Параметры: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'lmeds'). Изменение ошибки: -0.3370813471124708 px\n",
      "Параметры: (False, 'sift', 'FLANN', 'estimateAffinePartial2D', 'rho'). Изменение ошибки: 2.107374643590543 px\n",
      "Параметры: (False, 'kaze', 'FLANN', 'estimateAffine2D', 'lmeds'). Изменение ошибки: 4.813280121239494 px\n",
      "Параметры: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'ransac'). Изменение ошибки: 4.380965580026204 px\n",
      "Параметры: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'rho'). Изменение ошибки: 5.58905076531034 px\n",
      "Параметры: (False, 'sift', 'BF', 'estimateAffinePartial2D', 'lmeds'). Изменение ошибки: 3.7584155987830936 px\n",
      "Параметры: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'ransac'). Изменение ошибки: 2.3285823577790516 px\n",
      "Параметры: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'rho'). Изменение ошибки: 0.16853104066840185 px\n",
      "Параметры: (False, 'akaze', 'BF', 'estimateAffinePartial2D', 'lmeds'). Изменение ошибки: 2.6386546264770914 px\n",
      "Параметры: (True, 'sift', 'FLANN', 'estimateAffinePartial2D', 'rho'). Изменение ошибки: 0.32400042954086805 px\n",
      "Параметры: (True, 'sift', 'FLANN', 'findHomography', 'rho'). Изменение ошибки: 0.6708596813818035 px\n"
     ]
    }
   ],
   "source": [
    "best_results = results[:20]\n",
    "\n",
    "for result in best_results:\n",
    "    error, use_local_contrast_filter, detector, matching, matrix_type, matrix_estimate_method, _ = result\n",
    "    noised_error = homography_experenent(use_local_contrast_filter, detector, matching, matrix_type, matrix_estimate_method, True)\n",
    "    print(f\"Параметры: {result[1:-1]}. Изменение ошибки: {noised_error - error} px\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример хорошо построенной карты:  \n",
    "\n",
    "![Пример хорошо построенной карты](./assets/section2/builded_maps/builded_map_sift_BF_filtered-False_estimateAffinePartial2D_ransac.png)  \n",
    "\n",
    "Пример построенной карты с применением повышения контрастности:  \n",
    "\n",
    "![Пример построенной карты с применением повышения контрастности](./assets/section2/builded_maps/builded_map_sift_BF_filtered-True_estimateAffinePartial2D_ransac.png)\n",
    "\n",
    "Пример неудачно построенной карты:  \n",
    "\n",
    "![Пример неудачно построенной карты](./assets/section2/builded_maps/builded_map_orb_BF_filtered-False_findHomography_ransac.png)  \n",
    "\n",
    "[Посмотреть все сгенерированные карты](./assets/section2/builded_maps/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по 2 заданию\n",
    "- На мой взгляд лучший алгоритм по соотношению качество / скорость это AKAZE т.к. он достаточно быстрый (быстрее SIFT и KAZE), дал приемлемую ошибку и визуально построенная карта выглядит с относительно малым количеством артефактов. Худшим алгоритмом оказался ORB. Хотя он самый быстрый, но в большинстве случаев давал максимальную ошибку и больше всего артефактов при построении карты. Алгоритмы SIFT и KAZE показывают минимальную ошибку, однако вычислительно затратны (есть реализации на GPU)\n",
    "- Связка из методов поиска матрицы гомографии estimateAffine2D + lmeds дала меньше всего визуальных артефактов при построении карты\n",
    "- При добавлении шума результаты ухудшились, что ожидаемо.\n",
    "- 5 лучших результатов получены с использованием фильтра контрастности. Однако для этого видео его можно не использовать т.к. много контрастных элементов и хороших ключевых точек достаточно. \n",
    "- Для более точных результатов и автоматическому учету дисторсии камеры требуется взять инлайнеры например после RANSAC, построить более сложную модель с учетом дисторсии и находить матрицы гомографии через Ceres Solver\n",
    "- Не имеет смысла обрабатывать каждый кадр т.к. изменения в нем могут быть несущественными. Это замедляет обработку и увеличивает ошибку машинных вычислений. Для решения этой проблемы можно использовать более быстрый `cv2.calcOpticalFlowPyrLK` и задать трэшхолд например 10 пикселей.\n",
    "- Реальные кадры могут быть смазаны. Отсеять их можно например таким фильтром `cv2.Laplacian(frame, cv2.CV_64F).var() < 200`\n",
    "- Построили карты основываясь на движении камеры, оценили ошибку программно и визуально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "LoFTR  - любой другой, сравнить с готовыми решениями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ\n",
    "Воспользуемся алгоритмом LoFTR через библиотеку kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kornia in /home/itiv422/.local/lib/python3.10/site-packages (0.7.4)\n",
      "Requirement already satisfied: packaging in /home/itiv422/.local/lib/python3.10/site-packages (from kornia) (24.1)\n",
      "Requirement already satisfied: kornia-rs>=0.1.0 in /home/itiv422/.local/lib/python3.10/site-packages (from kornia) (0.1.7)\n",
      "Requirement already satisfied: torch>=1.9.1 in /home/itiv422/.local/lib/python3.10/site-packages (from kornia) (2.4.0+cu124)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (11.6.0.99)\n",
      "Requirement already satisfied: fsspec in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.99)\n",
      "Requirement already satisfied: sympy in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (1.12)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.4.99)\n",
      "Requirement already satisfied: filelock in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/itiv422/.local/lib/python3.10/site-packages (from torch>=1.9.1->kornia) (12.3.0.142)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/itiv422/.local/lib/python3.10/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/itiv422/.local/lib/python3.10/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_outdoor.ckpt\" to /home/itiv422/.cache/torch/hub/checkpoints/loftr_outdoor.ckpt\n",
      "100%|██████████| 44.2M/44.2M [02:11<00:00, 352kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: 61.5907910089599 px\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import kornia as K\n",
    "from kornia.feature import LoFTR\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "cap = cv2.VideoCapture('./assets/section2/0001-0480.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "is_first_frame = True\n",
    "output_map_size = (1280, 1800)\n",
    "builded_map = np.zeros((output_map_size[1], output_map_size[0]), dtype=np.uint8)\n",
    "loftr = LoFTR(pretrained='outdoor')\n",
    "H_accumulated = np.array([[1, 0, frame_width // 8],\n",
    "                        [0, 1, frame_height // 8],\n",
    "                        [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.resize(frame, (640, 480)) / 255.0\n",
    "\n",
    "    if is_first_frame:\n",
    "        prev_frame = frame\n",
    "        is_first_frame = False\n",
    "        continue\n",
    "    \n",
    "    frame_tensor = torch.from_numpy(frame).unsqueeze(0).unsqueeze(0).float()\n",
    "    prev_frame_tensor = torch.from_numpy(prev_frame).unsqueeze(0).unsqueeze(0).float()\n",
    "    input_dict = {\"image0\": frame_tensor, \"image1\": prev_frame_tensor}\n",
    "    with torch.no_grad():\n",
    "        correspondences = loftr(input_dict)\n",
    "\n",
    "    mkpts0 = torch.from_numpy(correspondences['keypoints0'].cpu().numpy()).reshape(1, -1, 2)\n",
    "    mkpts1 = torch.from_numpy(correspondences['keypoints1'].cpu().numpy()).reshape(1, -1, 2)\n",
    "    H = K.geometry.find_homography_dlt(mkpts0, mkpts1)\n",
    "    H_accumulated = H_accumulated @ H.squeeze().numpy()\n",
    "    warped_img = cv2.warpPerspective(frame * 255, H_accumulated, output_map_size, borderMode=cv2.BORDER_TRANSPARENT).astype(np.uint8)\n",
    "    mask = np.zeros_like(builded_map, dtype=np.uint8)\n",
    "    mask[warped_img > 0] = 1\n",
    "    builded_map = cv2.add(builded_map * (1 - mask), warped_img * mask)\n",
    "    prev_frame = frame\n",
    "\n",
    "cv2.imwrite(\"./assets/section3/builded_map_LoFTR.png\", builded_map)\n",
    "error = math.sqrt((H_accumulated[0, 2] - (frame_width // 8)) ** 2 + (H_accumulated[1, 2] - (frame_height // 8)) ** 2)\n",
    "\n",
    "print(f\"Ошибка: {error} px\")\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта:\n",
    "\n",
    "![Карта](./assets/section3/builded_map_LoFTR.png)\n",
    "\n",
    "### Выводы:\n",
    "- Запустили алгоритм, использующий нейронные сети.\n",
    "- LoFTR на CPU гораздо медленнее, чем классические алгоритмы из второго задания.\n",
    "- Для текущего датасета алгоритм показал очень плохой результат (ошибка около 60 пикселей хотя некоторые классические давали более точный результат)\n",
    "- Нейронные сети могут сопоставлять, например, ИК и визуальный канал (d2-net), что для классических алгоритмов очень сложно достичь.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
