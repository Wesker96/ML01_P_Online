{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для 1 любого видео восстановить траекторию движения (t-вектор). Выполнить визуализацию. Определить параметры которые влияют на \"точность\" определения вектора t. \n",
    "2. Использовать решение на базе нейронных сетей. Любые идеи. \n",
    "3. ***slam прикрутить. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм:\n",
    "1. Считываем по кадру из видео, каждый кадр преобразуем в черно-белый.\n",
    "2. Получаем особые точки и дескрипторы точек кадра.\n",
    "3. Сопоставляем точки с двух соседних кадров по их дискрипторам.\n",
    "4. Получаем оценочную матрицу по точкам двух кадров.\n",
    "5. Сохраняем матрицы поворота и смещения камеры, позиции камеры на каждом кадре.\n",
    "6. Ключевые точки и дескрипторы текущего кадра сохраняем для дальнейших вычислений на следующей итерации.\n",
    "7. Визуализирум траекторию положений камеры и ее направления в каждой точке.\n",
    "8. Сохраняем массивы с матрицами углов и смещений в npz-файл, т к очередное получение данных матриц занимает много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# function for vizualize cemera trajectory and direction\n",
    "def visualize_trajectory(rotation, positions, title='Camera motion'):\n",
    "    fig = gr.Figure()\n",
    "\n",
    "    # add positions trace\n",
    "    fig.add_trace(gr.Scatter3d(x=positions[:, 0], y=positions[:, 1], z=positions[:, 2],\n",
    "                               marker=dict(size=1.2, color='purple')))\n",
    "\n",
    "    # add camera orientation traces\n",
    "    for (p, r) in zip(positions, rotation):\n",
    "        point2 = p + 0.5 * r[:, 2]\n",
    "        fig.add_trace(gr.Scatter3d(x=[p[0], point2[0]], y=[p[1], point2[1]], z=[p[2], point2[2]],\n",
    "                                   mode='lines', line=dict(width=2, color='red')))\n",
    "\n",
    "    fig.update_layout(title=title, showlegend=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match features between images keypoints and filter matching points by distance\n",
    "def match_and_filtering(descriptor1, descriptor2, matcher, threshold: float):\n",
    "    filtered_matches = list()\n",
    "\n",
    "    # k best matches between descriptors\n",
    "    matches = matcher.knnMatch(descriptor1, descriptor2, k=2)\n",
    "\n",
    "    for m, n in matches:\n",
    "        if m.distance * 1. / n.distance <= threshold:\n",
    "            filtered_matches.append(m)\n",
    "\n",
    "    return filtered_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for create camera trajectory by video\n",
    "def process_video(source_video: str, method=cv2.LMEDS, threshold: float=1.):\n",
    "    cap = cv2.VideoCapture(source_video)\n",
    "\n",
    "    # create SIFT detector and BFMatcher objects for all video frames\n",
    "    sift_detector = cv2.SIFT_create()\n",
    "    matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "    # keypoints and descriptors from prev frame\n",
    "    kps1, des1 = None, None\n",
    "\n",
    "    # create_array with points of camera trajectory\n",
    "    trajectory = np.array([[0, 0, 0]])\n",
    "\n",
    "    # create general list with rotations matrix corresponds to each video frame\n",
    "    rotations_list = [np.zeros((3, 3))]\n",
    "    \n",
    "    # camera positions\n",
    "    positions = [np.array([0, 0, 0])]\n",
    "\n",
    "    cam_matrix = np.eye(4)\n",
    "    T = np.eye(4)\n",
    "\n",
    "    # camera matrix\n",
    "    K = np.array([[3000, 0 , cap.get(cv2.CAP_PROP_FRAME_WIDTH) / 2], [0, 3000, cap.get(cv2.CAP_PROP_FRAME_HEIGHT) / 2], [0, 0, 1]])\n",
    "\n",
    "    while True:\n",
    "        is_success, frame = cap.read()\n",
    "\n",
    "        if not is_success:\n",
    "            break\n",
    "\n",
    "        # convert frame to gray\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect frame keypoints\n",
    "        kps2, des2 = sift_detector.detectAndCompute(frame_gray, None)\n",
    "\n",
    "        if kps1 is not None:\n",
    "            # match keypoints from 2 frames, filtering by distance\n",
    "            matches = match_and_filtering(des1, des2, matcher, 0.3)\n",
    "\n",
    "            points1 = np.array([kps1[m.queryIdx].pt for m in matches])\n",
    "            points2 = np.array([kps2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "            # calculate essential matrix to match camera positons between 2 frames\n",
    "            e_mat, mask = cv2.findEssentialMat(points1, points2, K, method=method, threshold=threshold)\n",
    "            _, R, t, _ = cv2.recoverPose(e_mat, points1, points2, K, mask=mask)\n",
    "\n",
    "            T[:3, :3] = R\n",
    "            T[:3, 3] = t.T\n",
    "\n",
    "            cam_matrix = np.dot(cam_matrix, T)\n",
    "            trajectory = np.vstack([trajectory, cam_matrix[:3, 3]])\n",
    "            rotations_list.append(cam_matrix[:3, :3])\n",
    "            \n",
    "            positions.append(positions[-1] + np.dot(R, t).T[0])\n",
    "            \n",
    "        # save keypoints and descriptors from current frame to use in next calculations\n",
    "        kps1 = kps2\n",
    "        des1 = des2\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return rotations_list, trajectory, np.array(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_methods = {cv2.LMEDS: 'LMEDS', cv2.RANSAC: 'RANSAC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process single video\n",
    "def video_processor(video_path: str, method: int, threshold: float):\n",
    "    global data_folder, estimate_methods\n",
    "\n",
    "    if method == cv2.LMEDS:\n",
    "        threshold = 1.\n",
    "\n",
    "    # get camera rotations and trajectory\n",
    "    rotations_list, trajectory, positions = process_video(video_path, method, threshold)\n",
    "\n",
    "    # visualize camera motion, get fps value from path\n",
    "    start = video_path.rfind('_')\n",
    "    end = video_path.find('fps')\n",
    "    title = f'Camera motion, {estimate_methods[method]}-{threshold}, video fps = {video_path[start+1:end]}'\n",
    "    visualize_trajectory(rotations_list, trajectory, title)\n",
    "\n",
    "    # save camera rotations and trajectory to npz-file\n",
    "    npz_filename = os.path.join(data_folder, f'data_{estimate_methods[method]}-{threshold}_{video_path[start+1:end]}fps.npz')\n",
    "    np.savez(npz_filename, R=rotations_list, trajectory=trajectory, positions=positions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем видео с различными значением fps (fps = 5, 10, 20), чтобы подобрать видео с достаточным смещением пикселей между двумя кадрами. Также проверяем два метода для вычисления оценочной матрицы (cv2.LMEDS, cv2.RANSAC), и пороговые значения от 1 до 3ех для метода cv2.RANSAC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder that contains videos with fps = 5, 10, 20\n",
    "videos_folder = '/media/vika/SamsungSSD7/Courses/peleng-cources/HW_10/videos'\n",
    "\n",
    "# folder for saving npz-files with trajectory and rotations arrays\n",
    "data_folder = '/media/vika/SamsungSSD7/Courses/peleng-cources/HW_10/saved_data'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "\n",
    "# method's thresholds for find estimate matrix\n",
    "methods_ths = {cv2.LMEDS: [1.], cv2.RANSAC: [1., 2., 3.]}\n",
    "\n",
    "# iterate through videos with different fps and caclculate they trajectories\n",
    "for video_path in tqdm(os.listdir(videos_folder)):\n",
    "    print(video_path)\n",
    "\n",
    "    for method, thresholds in methods_ths.items():\n",
    "        for th in thresholds:\n",
    "            video_processor(os.path.join(videos_folder, video_path), method=method, threshold=th)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_filename = '/media/vik/SamsungSSD7/Courses/peleng-cources/HW_10/saved_data/data_LoFTR_LMEDS_10fps.npz'\n",
    "\n",
    "# load arrays from npz-file\n",
    "data = np.load(npz_filename)\n",
    "trajectory = data['trajectory']\n",
    "rotations_list = data['R']\n",
    "positions = data['positions']\n",
    "\n",
    "visualize_trajectory(rotations_list, positions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1pvf7TCdp9jB8ltiveLlwgNiyNFXYwaLZ/view?usp=sharing\">Видео с fps=5</a> и графики траектории для параметров: method=LMEDS, threshold=1.0; method=RANSAC, threshold=1.0; method=RANSAC, threshold=2.0; method=RANSAC, threshold=3.0 соответственно:<br>\n",
    "![img](./plots/plot_LMEDS_5fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-1_5fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-2_5fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-3_5fps.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1jcTTa6b_1FG8GAi6ChaRziC0N0UpmVvZ/view?usp=drive_link\">Видео с fps=10</a> и графики траектории для параметров: method=LMEDS, threshold=1.0; method=RANSAC, threshold=1.0; method=RANSAC, threshold=2.0; method=RANSAC, threshold=3.0 соответственно:<br>\n",
    "![img](./plots/plot_LMEDS_10fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-1_10fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-2_10fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-3_10fps.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1oq_rdvVcYy58YdZAngiSzLht09p8KREE/view?usp=sharing\">Видео с fps=20</a> и графики траектории для параметров: method=LMEDS, threshold=1.0; method=RANSAC, threshold=1.0; method=RANSAC, threshold=2.0; method=RANSAC, threshold=3.0 соответственно:<br>\n",
    "![img](./plots/plot_LMEDS_20fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-1_20fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-2_20fps.png)<br>\n",
    "![img](./plots/plot_RANSAC-3_20fps.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1pvf7TCdp9jB8ltiveLlwgNiyNFXYwaLZ/view?usp=drive_link\">Видео с fps=5</a> и график траектории, полученный с помощью нейронной сети LoFTR и метода cv2.LMEDS:<br>\n",
    "![img](./plots/plot_LoFTR_5fps.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графикам, полученным для различных комбинаций fps и методов, видно, что ближайшей является траектория при fps == 5, методом вычисления матрицы cv2.LMEDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На точность построения траектории оказывают влияние пороговые значения для фильтрации точек после их сопоставления на соседних кадрах, смещение между соседними кадрами (различное fps), выбор метода для вычисления оценочной матрицы (cv2.LMEDS, cv2.RANSAC) в комбинации с порогами для данных методов (от 1 до 3ех для метода cv2.RANSAC), разрешение кадров видео. Точность построения траектории при использовании модели LoFTR оказалась хуже точности SIFT для данного видео. Ее работа требует больших ресурсов по памяти, поэтому нет возможности подать на вход картинку в полном разрешении, также на CUDA модель работает медленнее, чем SIFT. Ниже приведены траектории, построенные с использованием SIFT и LoFTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
