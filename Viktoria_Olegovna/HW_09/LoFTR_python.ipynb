{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10566,
     "status": "ok",
     "timestamp": 1733682970021,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "odzx7sIE-COT",
    "outputId": "bda12c46-8cce-4534-a654-7daf9225f51e"
   },
   "outputs": [],
   "source": [
    "!pip3 install kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc2b5-IaIN8G"
   },
   "source": [
    "3) LoFTR  - любой другой, сравнить с готовыми решениями. <br>\n",
    "Напишите Вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70498,
     "status": "ok",
     "timestamp": 1733684417384,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "CVInu2RczqLy",
    "outputId": "0524a5b8-5cd8-44d7-f43b-66d1344485da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1733683083261,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "v_NNRGAYyiM5"
   },
   "outputs": [],
   "source": [
    "from kornia.geometry import find_homography_dlt\n",
    "from kornia.feature import LoFTR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1733684865884,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "NlqsaWyV-Ixw"
   },
   "outputs": [],
   "source": [
    "# use LoFTR matcher to get general points from two frames\n",
    "def process_imgs(matcher, input_dict: dict):\n",
    "  with torch.inference_mode():\n",
    "    corrs = matcher(input_dict)\n",
    "\n",
    "  kps1 = corrs[\"keypoints0\"].reshape(1, -1, 2)\n",
    "  kps2 = corrs[\"keypoints1\"].reshape(1, -1, 2)\n",
    "\n",
    "  return kps1, kps2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1733685004948,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "B550VaJU-YsY"
   },
   "outputs": [],
   "source": [
    "# functin to find point p1 from first video frame in last frame\n",
    "def processing_video(output_path: str, p1):\n",
    "    global INPUT_VIDEO_PATH\n",
    "\n",
    "    # create empty affine matrix to save affine matrices multiplication results\n",
    "    # from whole video\n",
    "    general_matrix_3d = np.array([], dtype=float)\n",
    "    prev_frame_cuda = None\n",
    "\n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "    input_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    input_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "    RESIZED_RESOLUTION = (810, 544)\n",
    "    # scales for image reduction\n",
    "    scale_x = input_width / RESIZED_RESOLUTION[0]\n",
    "    scale_y = input_height / RESIZED_RESOLUTION[1]\n",
    "\n",
    "    p1[:2] = [p1[0] / scale_x, p1[1] / scale_y]\n",
    "\n",
    "    # create video writer to make video with point\n",
    "    fourcc = cv2.VideoWriter.fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS),\n",
    "                                   (int(input_width), int(input_height)))\n",
    "\n",
    "    # create Local Feature Matching with Transformers\n",
    "    matcher = LoFTR(pretrained='indoor_new')\n",
    "    matcher = matcher.eval().cuda()\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # convert frame to one-dimential gray image, than resize\n",
    "        resized_frame_gray = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),\n",
    "                                        RESIZED_RESOLUTION, cv2.INTER_CUBIC)\n",
    "\n",
    "        # reshape image to get shape (batch x channels x H x W)\n",
    "        resized_frame_gray = resized_frame_gray.reshape(1, 1,\n",
    "                                                        RESIZED_RESOLUTION[1],\n",
    "                                                        RESIZED_RESOLUTION[0])\n",
    "        frame_cuda = torch.from_numpy(resized_frame_gray).cuda() / 255.\n",
    "\n",
    "        if prev_frame_cuda is not None:\n",
    "            input_dict = {'image0': prev_frame_cuda, 'image1': frame_cuda}\n",
    "            kps1, kps2 = process_imgs(matcher, input_dict)\n",
    "\n",
    "            # get affine 3D-matrix between two alternate frames\n",
    "            affine_matrix_3d = find_homography_dlt(kps1, kps2).cpu().numpy()[0]\n",
    "            general_matrix_3d = (affine_matrix_3d if general_matrix_3d.size == 0\n",
    "                                 else np.dot(affine_matrix_3d, general_matrix_3d))\n",
    "\n",
    "            # calculate p1 coordinates in current frame, for visualization only\n",
    "            p2 = np.dot(general_matrix_3d, p1)\n",
    "            frame = cv2.circle(frame, (int(p2[0] * scale_x),\n",
    "                                       int(p2[1] * scale_y)),\n",
    "                               10, (0, 0, 255), -1)\n",
    "        else:\n",
    "            frame = cv2.circle(frame, (int(p1[0] * scale_x),\n",
    "                                       int(p1[1] * scale_y)),\n",
    "                               10, (0, 0, 255), -1)\n",
    "        video_writer.write(frame)\n",
    "\n",
    "        # save current resized gray frame as previous to calculate affine matrix in next step\n",
    "        prev_frame_cuda = frame_cuda\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "\n",
    "    # rescale p2 point\n",
    "    p2[:2] = [p2[0] * scale_x, p2[1] * scale_y]\n",
    "\n",
    "    return p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 853377,
     "status": "ok",
     "timestamp": 1733685860806,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "Xb8CKKjF3LQl"
   },
   "outputs": [],
   "source": [
    "INPUT_VIDEO_PATH = '/content/source_video.mp4'\n",
    "OUTPUT_VIDEO_PATH = '/content/drive/MyDrive/video_with_point'\n",
    "\n",
    "df_columns = ['Descriptor method', 'Time,s', 'Error,px']\n",
    "df_file_path = '/content/drive/MyDrive/LoFRT_video_analyze.xlsx'\n",
    "analyze_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "p1 = [1430, 470, 1]\n",
    "p2_target = [1020, 400, 1]\n",
    "\n",
    "output_path = OUTPUT_VIDEO_PATH + '_' + 'LoFTR.mp4'\n",
    "\n",
    "start_t = time.time()\n",
    "p2 = processing_video(output_path, p1.copy())\n",
    "delta_t = round(time.time() - start_t, 4)\n",
    "\n",
    "points_dist = np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "error_dist = np.sqrt((p2[0] - p2_target[0]) ** 2 + (p2[1] - p2_target[1]) ** 2)\n",
    "\n",
    "df_row = ['LoFTR', delta_t, round(error_dist, 4)]\n",
    "\n",
    "if analyze_df.empty:\n",
    "    analyze_df = pd.DataFrame([df_row], columns=df_columns)\n",
    "else:\n",
    "    analyze_df = analyze_df._append(dict(zip(df_columns, df_row)),\n",
    "                                    ignore_index=True)\n",
    "analyze_df.to_excel(df_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1733685860809,
     "user": {
      "displayName": "Никита S",
      "userId": "17392445598705814809"
     },
     "user_tz": -180
    },
    "id": "95XPuh_e3LHs",
    "outputId": "cb680657-8f93-4984-ff8a-b5cd45bcdce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Descriptor method    Time,s   Error,px\n",
      "0             LoFTR  853.0437  8166.7404\n"
     ]
    }
   ],
   "source": [
    "print(analyze_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3xCIzSEiWaA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1BgNIOjFHauFoNB95LGesHBIjioX74USW",
     "timestamp": 1733602876321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
